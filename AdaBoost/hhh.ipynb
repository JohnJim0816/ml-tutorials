{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-38e1121362a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;31m# testDataList, testLabelList = loadData('../Mnist/mnist_test.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtrainDataList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabelListtrainDataList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabelList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtestDataList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabelListtestDataList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabelList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m#创建提升树\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "数据集：Mnist\n",
    "训练集数量：60000(实际使用：10000)\n",
    "测试集数量：10000（实际使用：1000)\n",
    "层数：40\n",
    "------------------------------\n",
    "运行结果：\n",
    "    正确率：97%\n",
    "    运行时长：65m\n",
    "'''\n",
    "\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "def load_data():  # categorical_crossentropy\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    number = 10000\n",
    "    x_train = x_train[0:number]\n",
    "    y_train = y_train[0:number]\n",
    "    x_train = x_train.reshape(number, 28 * 28)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28 * 28)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "    x_train = x_train\n",
    "    x_test = x_test\n",
    "    x_test = np.random.normal(x_test)  # 加噪声\n",
    "    \n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def loadData(fileName):\n",
    "    '''\n",
    "    加载文件\n",
    "    :param fileName:要加载的文件路径\n",
    "    :return: 数据集和标签集\n",
    "    '''\n",
    "    #存放数据及标记\n",
    "    dataArr = []; labelArr = []\n",
    "    #读取文件\n",
    "    fr = open(fileName)\n",
    "    #遍历文件中的每一行\n",
    "    for line in fr.readlines():\n",
    "        #获取当前行，并按“，”切割成字段放入列表中\n",
    "        #strip：去掉每行字符串首尾指定的字符（默认空格或换行符）\n",
    "        #split：按照指定的字符将字符串切割成每个字段，返回列表形式\n",
    "        curLine = line.strip().split(',')\n",
    "        #将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）\n",
    "        #在放入的同时将原先字符串形式的数据转换为整型\n",
    "        #此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算\n",
    "        dataArr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "        #将标记信息放入标记集中\n",
    "        #放入的同时将标记转换为整型\n",
    "\n",
    "        #转换成二分类任务\n",
    "        #标签0设置为1，反之为-1\n",
    "        if int(curLine[0]) == 0:\n",
    "            labelArr.append(1)\n",
    "        else:\n",
    "            labelArr.append(-1)\n",
    "    #返回数据集和标记\n",
    "    return dataArr, labelArr\n",
    "\n",
    "def calc_e_Gx(trainDataArr, trainLabelArr, n, div, rule, D):\n",
    "    '''\n",
    "    计算分类错误率\n",
    "    :param trainDataArr:训练数据集数字\n",
    "    :param trainLabelArr: 训练标签集数组\n",
    "    :param n: 要操作的特征\n",
    "    :param div:划分点\n",
    "    :param rule:正反例标签\n",
    "    :param D:权值分布D\n",
    "    :return:预测结果， 分类误差率\n",
    "    '''\n",
    "    #初始化分类误差率为0\n",
    "    e = 0\n",
    "    #将训练数据矩阵中特征为n的那一列单独剥出来做成数组。因为其他元素我们并不需要，\n",
    "    #直接对庞大的训练集进行操作的话会很慢\n",
    "    x = trainDataArr[:, n]\n",
    "    #同样将标签也转换成数组格式，x和y的转换只是单纯为了提高运行速度\n",
    "    #测试过相对直接操作而言性能提升很大\n",
    "    y = trainLabelArr\n",
    "    predict = []\n",
    "\n",
    "    #依据小于和大于的标签依据实际情况会不同，在这里直接进行设置\n",
    "    if rule == 'LisOne':    L = 1; H = -1\n",
    "    else:                   L = -1; H = 1\n",
    "\n",
    "    #遍历所有样本的特征m\n",
    "    for i in range(trainDataArr.shape[0]):\n",
    "        if x[i] < div:\n",
    "            #如果小于划分点，则预测为L\n",
    "            #如果设置小于div为1，那么L就是1，\n",
    "            #如果设置小于div为-1，L就是-1\n",
    "            predict.append(L)\n",
    "            #如果预测错误，分类错误率要加上该分错的样本的权值（8.1式）\n",
    "            if y[i] != L: e += D[i]\n",
    "        elif x[i] >= div:\n",
    "            #与上面思想一样\n",
    "            predict.append(H)\n",
    "            if y[i] != H: e += D[i]\n",
    "    #返回预测结果和分类错误率e\n",
    "    #预测结果其实是为了后面做准备的，在算法8.1第四步式8.4中exp内部有个Gx，要用在那个地方\n",
    "    #以此来更新新的D\n",
    "    return np.array(predict), e\n",
    "\n",
    "def createSigleBoostingTree(trainDataArr, trainLabelArr, D):\n",
    "    '''\n",
    "    创建单层提升树\n",
    "    :param trainDataArr:训练数据集数组\n",
    "    :param trainLabelArr: 训练标签集数组\n",
    "    :param D: 算法8.1中的D\n",
    "    :return: 创建的单层提升树\n",
    "    '''\n",
    "\n",
    "    #获得样本数目及特征数量\n",
    "    m, n = np.shape(trainDataArr)\n",
    "    #单层树的字典，用于存放当前层提升树的参数\n",
    "    #也可以认为该字典代表了一层提升树\n",
    "    sigleBoostTree = {}\n",
    "    #初始化分类误差率，分类误差率在算法8.1步骤（2）（b）有提到\n",
    "    #误差率最高也只能100%，因此初始化为1\n",
    "    sigleBoostTree['e'] = 1\n",
    "\n",
    "    #对每一个特征进行遍历，寻找用于划分的最合适的特征\n",
    "    for i in range(n):\n",
    "        #因为特征已经经过二值化，只能为0和1，因此分切分时分为-0.5， 0.5， 1.5三挡进行切割\n",
    "        for div in [-0.5, 0.5, 1.5]:\n",
    "            #在单个特征内对正反例进行划分时，有两种情况：\n",
    "            #可能是小于某值的为1，大于某值得为-1，也可能小于某值得是-1，反之为1\n",
    "            #因此在寻找最佳提升树的同时对于两种情况也需要遍历运行\n",
    "            #LisOne：Low is one：小于某值得是1\n",
    "            #HisOne：High is one：大于某值得是1\n",
    "            for rule in ['LisOne', 'HisOne']:\n",
    "                #按照第i个特征，以值div进行切割，进行当前设置得到的预测和分类错误率\n",
    "                Gx, e = calc_e_Gx(trainDataArr, trainLabelArr, i, div, rule, D)\n",
    "                #如果分类错误率e小于当前最小的e，那么将它作为最小的分类错误率保存\n",
    "                if e < sigleBoostTree['e']:\n",
    "                    sigleBoostTree['e'] = e\n",
    "                    #同时也需要存储最优划分点、划分规则、预测结果、特征索引\n",
    "                    #以便进行D更新和后续预测使用\n",
    "                    sigleBoostTree['div'] = div\n",
    "                    sigleBoostTree['rule'] = rule\n",
    "                    sigleBoostTree['Gx'] = Gx\n",
    "                    sigleBoostTree['feature'] = i\n",
    "    #返回单层的提升树\n",
    "    return sigleBoostTree\n",
    "\n",
    "def createBosstingTree(trainDataList, trainLabelList, treeNum = 50):\n",
    "    '''\n",
    "    创建提升树\n",
    "    创建算法依据“8.1.2 AdaBoost算法” 算法8.1\n",
    "    :param trainDataList:训练数据集\n",
    "    :param trainLabelList: 训练测试集\n",
    "    :param treeNum: 树的层数\n",
    "    :return: 提升树\n",
    "    '''\n",
    "    #将数据和标签转化为数组形式\n",
    "    trainDataArr = np.array(trainDataList)\n",
    "    trainLabelArr = np.array(trainLabelList)\n",
    "    #没增加一层数后，当前最终预测结果列表\n",
    "    finallpredict = [0] * len(trainLabelArr)\n",
    "    #获得训练集数量以及特征个数\n",
    "    m, n = np.shape(trainDataArr)\n",
    "\n",
    "    #依据算法8.1步骤（1）初始化D为1/N\n",
    "    D = [1 / m] * m\n",
    "    #初始化提升树列表，每个位置为一层\n",
    "    tree = []\n",
    "    #循环创建提升树\n",
    "    for i in range(treeNum):\n",
    "        #得到当前层的提升树\n",
    "        curTree = createSigleBoostingTree(trainDataArr, trainLabelArr, D)\n",
    "        #根据式8.2计算当前层的alpha\n",
    "        alpha = 1/2 * np.log((1 - curTree['e']) / curTree['e'])\n",
    "        #获得当前层的预测结果，用于下一步更新D\n",
    "        Gx = curTree['Gx']\n",
    "        #依据式8.4更新D\n",
    "        #考虑到该式每次只更新D中的一个w，要循环进行更新知道所有w更新结束会很复杂（其实\n",
    "        #不是时间上的复杂，只是让人感觉每次单独更新一个很累），所以该式以向量相乘的形式，\n",
    "        #一个式子将所有w全部更新完。\n",
    "        #该式需要线性代数基础，如果不太熟练建议补充相关知识，当然了，单独更新w也一点问题\n",
    "        #没有\n",
    "        #np.multiply(trainLabelArr, Gx)：exp中的y*Gm(x)，结果是一个行向量，内部为yi*Gm(xi)\n",
    "        #np.exp(-1 * alpha * np.multiply(trainLabelArr, Gx))：上面求出来的行向量内部全体\n",
    "        #成员再乘以-αm，然后取对数，和书上式子一样，只不过书上式子内是一个数，这里是一个向量\n",
    "        #D是一个行向量，取代了式中的wmi，然后D求和为Zm\n",
    "        #书中的式子最后得出来一个数w，所有数w组合形成新的D\n",
    "        #这里是直接得到一个向量，向量内元素是所有的w\n",
    "        #本质上结果是相同的\n",
    "        D = np.multiply(D, np.exp(-1 * alpha * np.multiply(trainLabelArr, Gx))) / sum(D)\n",
    "        #在当前层参数中增加alpha参数，预测的时候需要用到\n",
    "        curTree['alpha'] = alpha\n",
    "        #将当前层添加到提升树索引中。\n",
    "        tree.append(curTree)\n",
    "\n",
    "        #-----以下代码用来辅助，可以去掉---------------\n",
    "        #根据8.6式将结果加上当前层乘以α，得到目前的最终输出预测\n",
    "        finallpredict += alpha * Gx\n",
    "        #计算当前最终预测输出与实际标签之间的误差\n",
    "        error = sum([1 for i in range(len(trainDataList)) if np.sign(finallpredict[i]) != trainLabelArr[i]])\n",
    "        #计算当前最终误差率\n",
    "        finallError = error / len(trainDataList)\n",
    "        #如果误差为0，提前退出即可，因为没有必要再计算算了\n",
    "        if finallError == 0:    return tree\n",
    "        #打印一些信息\n",
    "        print('iter:%d:%d, sigle error:%.4f, finall error:%.4f'%(i, treeNum, curTree['e'], finallError ))\n",
    "    #返回整个提升树\n",
    "    return tree\n",
    "\n",
    "def predict(x, div, rule, feature):\n",
    "    '''\n",
    "    输出单独层预测结果\n",
    "    :param x: 预测样本\n",
    "    :param div: 划分点\n",
    "    :param rule: 划分规则\n",
    "    :param feature: 进行操作的特征\n",
    "    :return:\n",
    "    '''\n",
    "    #依据划分规则定义小于及大于划分点的标签\n",
    "    if rule == 'LisOne':    L = 1; H = -1\n",
    "    else:                   L = -1; H = 1\n",
    "\n",
    "    #判断预测结果\n",
    "    if x[feature] < div: return L\n",
    "    else:   return H\n",
    "\n",
    "def model_test(testDataList, testLabelList, tree):\n",
    "    '''\n",
    "    测试\n",
    "    :param testDataList:测试数据集\n",
    "    :param testLabelList: 测试标签集\n",
    "    :param tree: 提升树\n",
    "    :return: 准确率\n",
    "    '''\n",
    "    #错误率计数值\n",
    "    errorCnt = 0\n",
    "    #遍历每一个测试样本\n",
    "    for i in range(len(testDataList)):\n",
    "        #预测结果值，初始为0\n",
    "        result = 0\n",
    "        #依据算法8.1式8.6\n",
    "        #预测式子是一个求和式，对于每一层的结果都要进行一次累加\n",
    "        #遍历每层的树\n",
    "        for curTree in tree:\n",
    "            #获取该层参数\n",
    "            div = curTree['div']\n",
    "            rule = curTree['rule']\n",
    "            feature = curTree['feature']\n",
    "            alpha = curTree['alpha']\n",
    "            #将当前层结果加入预测中\n",
    "            result += alpha * predict(testDataList[i], div, rule, feature)\n",
    "        #预测结果取sign值，如果大于0 sign为1，反之为0\n",
    "        if np.sign(result) != testLabelList[i]: errorCnt += 1\n",
    "    #返回准确率\n",
    "    return 1 - errorCnt / len(testDataList)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #开始时间\n",
    "    start = time.time()\n",
    "\n",
    "    # # 获取训练集\n",
    "    # print('start read transSet')\n",
    "    # trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv')\n",
    "\n",
    "    # # 获取测试集\n",
    "    # print('start read testSet')\n",
    "    # testDataList, testLabelList = loadData('../Mnist/mnist_test.csv')\n",
    "\n",
    "    (trainDataList, trainLabelList), (testDataList, testLabelList) = load_data()\n",
    "\n",
    "    #创建提升树\n",
    "    print('start init train')\n",
    "    tree = createBosstingTree(trainDataList[:10000], trainLabelList[:10000], 40)\n",
    "\n",
    "    #测试\n",
    "    print('start to test')\n",
    "    accuracy = model_test(testDataList[:1000], testLabelList[:1000], tree)\n",
    "    print('the accuracy is:%d' % (accuracy * 100), '%')\n",
    "\n",
    "    #结束时间\n",
    "    end = time.time()\n",
    "    print('time span:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bitba104053253c43d2b2643fcd808cd6d9",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}